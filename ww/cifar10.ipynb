{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import resnet1,trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda')\n",
    "def fit(model:resnet1.ResNet, optim,lossfunc,trainloader: DataLoader):\n",
    "    model.train()\n",
    "    totalloss = 0\n",
    "    for data, target in trainloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optim.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = lossfunc(output, target)\n",
    "        with torch.no_grad():\n",
    "            totalloss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),3.0)\n",
    "        optim.step()\n",
    "    return totalloss\n",
    "\n",
    "def evaluate(model:ResNet.ResNet20, val_loader: DataLoader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='/d/usr14/project/Binary/wwdata', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "train_size = 45000\n",
    "val_size = 5000\n",
    "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128,\n",
    "                                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='/d/usr14/project/Binary/wwdata', train=False,\n",
    "                                       download=False, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n",
      "mnist, mode: n\n"
     ]
    }
   ],
   "source": [
    "bw=ba=False\n",
    "print(bw, ba)\n",
    "savepath = \"D:/usr14/project/Binary/wwdata/cifa10/fullbest.pth\"\n",
    "if bw and not ba:\n",
    "    mode = \"w\"\n",
    "elif ba and not bw:\n",
    "    mode = \"a\"\n",
    "else:\n",
    "    mode = \"b\"\n",
    "mode=\"n\"\n",
    "print(\"mnist, mode: \" + mode)\n",
    "model = ResNet.ResNet20(bw, ba).to(device)\n",
    "lossfunc=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 527.3771959543228, train_acc:0.5704222222222223, val_acc:0.5734\n",
      "epoch: 1, loss: 380.9333062171936, train_acc:0.6572222222222223, val_acc:0.6526\n",
      "epoch: 2, loss: 332.5422064065933, train_acc:0.6764, val_acc:0.6646\n",
      "epoch: 3, loss: 307.262766122818, train_acc:0.7109111111111112, val_acc:0.6896\n",
      "epoch: 4, loss: 282.11122596263885, train_acc:0.719, val_acc:0.701\n",
      "epoch: 5, loss: 265.2082188129425, train_acc:0.7427333333333334, val_acc:0.7228\n",
      "epoch: 6, loss: 255.88003078103065, train_acc:0.763, val_acc:0.7436\n",
      "epoch: 7, loss: 240.11122804880142, train_acc:0.7549555555555556, val_acc:0.7302\n",
      "epoch: 8, loss: 232.33611372113228, train_acc:0.7801777777777777, val_acc:0.754\n",
      "epoch: 9, loss: 222.4561694264412, train_acc:0.8011111111111111, val_acc:0.7714\n",
      "epoch: 10, loss: 158.4303584396839, train_acc:0.8647111111111111, val_acc:0.8194\n",
      "epoch: 11, loss: 141.23211106657982, train_acc:0.8769333333333333, val_acc:0.8266\n",
      "epoch: 12, loss: 133.71970984339714, train_acc:0.8825555555555555, val_acc:0.8236\n",
      "epoch: 13, loss: 126.46457634866238, train_acc:0.8897777777777778, val_acc:0.8276\n",
      "epoch: 14, loss: 121.53716787695885, train_acc:0.8903555555555556, val_acc:0.8312\n",
      "epoch: 15, loss: 116.21847105026245, train_acc:0.8760888888888889, val_acc:0.8078\n",
      "epoch: 16, loss: 111.40682652592659, train_acc:0.895, val_acc:0.827\n",
      "epoch: 17, loss: 107.93134093284607, train_acc:0.9028222222222222, val_acc:0.8296\n",
      "epoch: 18, loss: 103.66949100792408, train_acc:0.8934444444444445, val_acc:0.8192\n",
      "epoch: 19, loss: 99.12433291971684, train_acc:0.9210666666666667, val_acc:0.8342\n",
      "epoch: 20, loss: 79.35828479379416, train_acc:0.9364222222222223, val_acc:0.842\n",
      "epoch: 21, loss: 74.02273151278496, train_acc:0.9385777777777777, val_acc:0.842\n",
      "epoch: 22, loss: 72.26640275120735, train_acc:0.9347777777777778, val_acc:0.8396\n",
      "epoch: 23, loss: 69.99698787927628, train_acc:0.9387555555555556, val_acc:0.8382\n",
      "epoch: 24, loss: 68.9715573117137, train_acc:0.9382222222222222, val_acc:0.8382\n",
      "epoch: 25, loss: 68.27781334519386, train_acc:0.9453777777777778, val_acc:0.8416\n",
      "epoch: 26, loss: 66.75166761130095, train_acc:0.9232666666666667, val_acc:0.829\n",
      "epoch: 27, loss: 65.36927583813667, train_acc:0.9405555555555556, val_acc:0.8388\n",
      "epoch: 28, loss: 64.79468946158886, train_acc:0.9376666666666666, val_acc:0.8326\n",
      "epoch: 29, loss: 63.49888067692518, train_acc:0.9533555555555555, val_acc:0.841\n",
      "epoch: 30, loss: 60.73315817117691, train_acc:0.9234666666666667, val_acc:0.828\n",
      "epoch: 31, loss: 60.540370769798756, train_acc:0.9437333333333333, val_acc:0.836\n",
      "epoch: 32, loss: 60.05733823776245, train_acc:0.9526444444444444, val_acc:0.8448\n",
      "epoch: 33, loss: 59.39738566428423, train_acc:0.9466888888888889, val_acc:0.8398\n",
      "epoch: 34, loss: 60.33756306767464, train_acc:0.9358888888888889, val_acc:0.832\n",
      "epoch: 35, loss: 59.30730115622282, train_acc:0.9403555555555555, val_acc:0.8316\n",
      "epoch: 36, loss: 58.90608052164316, train_acc:0.9559555555555556, val_acc:0.843\n",
      "epoch: 37, loss: 59.00225514918566, train_acc:0.9393111111111111, val_acc:0.8332\n",
      "epoch: 38, loss: 58.96526765078306, train_acc:0.9539333333333333, val_acc:0.842\n",
      "epoch: 39, loss: 59.09709403663874, train_acc:0.9508222222222222, val_acc:0.8378\n",
      "epoch: 40, loss: 58.86957313120365, train_acc:0.9565555555555556, val_acc:0.8422\n",
      "epoch: 41, loss: 59.00812640041113, train_acc:0.9407333333333333, val_acc:0.8376\n",
      "epoch: 42, loss: 58.643250711262226, train_acc:0.9448222222222222, val_acc:0.838\n",
      "epoch: 43, loss: 58.53833819925785, train_acc:0.9510444444444445, val_acc:0.84\n",
      "epoch: 44, loss: 58.598260916769505, train_acc:0.9358222222222222, val_acc:0.8344\n",
      "epoch: 45, loss: 58.81113436073065, train_acc:0.9424222222222223, val_acc:0.836\n",
      "epoch: 46, loss: 58.624867398291826, train_acc:0.9474, val_acc:0.8408\n",
      "epoch: 47, loss: 58.97208468616009, train_acc:0.9348888888888889, val_acc:0.8334\n",
      "epoch: 48, loss: 58.89311671257019, train_acc:0.9389555555555555, val_acc:0.8308\n",
      "epoch: 49, loss: 59.012925647199154, train_acc:0.9307111111111112, val_acc:0.8296\n",
      "epoch: 50, loss: 58.670863926410675, train_acc:0.9493111111111111, val_acc:0.8416\n",
      "epoch: 51, loss: 59.01198273897171, train_acc:0.9458888888888889, val_acc:0.8346\n",
      "epoch: 52, loss: 58.76409001648426, train_acc:0.9556444444444444, val_acc:0.84\n",
      "epoch: 53, loss: 58.92872750014067, train_acc:0.9512666666666667, val_acc:0.8414\n",
      "epoch: 54, loss: 58.17486930638552, train_acc:0.9491333333333334, val_acc:0.8384\n",
      "epoch: 55, loss: 58.67442152649164, train_acc:0.9483777777777778, val_acc:0.8364\n",
      "epoch: 56, loss: 58.52651658654213, train_acc:0.9563777777777778, val_acc:0.8436\n",
      "epoch: 57, loss: 58.68942213058472, train_acc:0.9526444444444444, val_acc:0.8422\n",
      "epoch: 58, loss: 57.95026137679815, train_acc:0.9500888888888889, val_acc:0.8388\n",
      "epoch: 59, loss: 58.305294528603554, train_acc:0.9576, val_acc:0.8448\n",
      "epoch: 60, loss: 58.542664021253586, train_acc:0.9387111111111112, val_acc:0.8344\n",
      "epoch: 61, loss: 58.37136201560497, train_acc:0.9510888888888889, val_acc:0.8394\n",
      "epoch: 62, loss: 58.325627103447914, train_acc:0.9436666666666667, val_acc:0.8378\n",
      "epoch: 63, loss: 58.69413781166077, train_acc:0.9509333333333333, val_acc:0.8392\n",
      "epoch: 64, loss: 58.30476550012827, train_acc:0.9481777777777778, val_acc:0.8396\n",
      "epoch: 65, loss: 58.39060318470001, train_acc:0.9544, val_acc:0.8444\n",
      "epoch: 66, loss: 58.88350486010313, train_acc:0.9506222222222223, val_acc:0.8426\n",
      "epoch: 67, loss: 58.89662383496761, train_acc:0.9505111111111111, val_acc:0.8416\n",
      "epoch: 68, loss: 58.61940635740757, train_acc:0.9418222222222222, val_acc:0.8376\n",
      "epoch: 69, loss: 58.96603259444237, train_acc:0.9543333333333334, val_acc:0.8428\n",
      "epoch: 70, loss: 58.845381282269955, train_acc:0.9230666666666667, val_acc:0.8206\n",
      "epoch: 71, loss: 58.374314434826374, train_acc:0.9433777777777778, val_acc:0.835\n",
      "epoch: 72, loss: 58.83235029131174, train_acc:0.9519777777777778, val_acc:0.8398\n",
      "epoch: 73, loss: 58.342911787331104, train_acc:0.9459333333333333, val_acc:0.839\n",
      "epoch: 74, loss: 58.80675361305475, train_acc:0.9459111111111111, val_acc:0.8368\n",
      "epoch: 75, loss: 58.539356119930744, train_acc:0.9332, val_acc:0.8304\n",
      "epoch: 76, loss: 57.954084888100624, train_acc:0.9536444444444444, val_acc:0.842\n",
      "epoch: 77, loss: 57.93071134388447, train_acc:0.9503333333333334, val_acc:0.8402\n",
      "epoch: 78, loss: 58.36318112164736, train_acc:0.9491777777777778, val_acc:0.8392\n",
      "epoch: 79, loss: 58.17797463387251, train_acc:0.9450888888888889, val_acc:0.8362\n"
     ]
    }
   ],
   "source": [
    "lr=1\n",
    "for epoch in range(80):\n",
    "    if epoch%10==0:\n",
    "        lr/=10\n",
    "        optim=torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    loss=fit(model,optim,lossfunc,train_loader)\n",
    "    tacc=evaluate(model,train_loader)\n",
    "    acc=evaluate(model,val_loader)\n",
    "    print(f'epoch: {epoch}, loss: {loss}, train_acc:{tacc}, val_acc:{acc}')\n",
    "torch.save(model.state_dict(), 'pre1.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
